{"version":3,"sources":["webpack:///path---generating-text-using-an-lstm-neural-network-14023ea4cdd2b17700fd.js","webpack:///./.cache/json/generating-text-using-an-lstm-neural-network.json"],"names":["webpackJsonp","465","module","exports","data","markdownRemark","html","timeToRead","excerpt","frontmatter","title","cover","date","category","tags","fields","nextTitle","nextSlug","prevTitle","prevSlug","slug","pathContext"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,w9OAAmgKC,WAAA,EAAAC,QAAA,+IAAAC,aAAmpFC,MAAA,+CAAAC,MAAA,+BAAAC,KAAA,mBAAAC,SAAA,0BAAAC,MAAA,mCAAuMC,QAAWC,UAAA,uBAAAC,SAAA,0BAAAC,UAAA,yCAAAC,SAAA,0CAAAC,KAAA,mDAA2OC,aAAgBD,KAAA","file":"path---generating-text-using-an-lstm-neural-network-14023ea4cdd2b17700fd.js","sourcesContent":["webpackJsonp([269976026368624],{\n\n/***/ 465:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<p>Some time ago I read Karpathy's <a href=\\\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\\">article</a> on LSTM networks and text generation. The idea that AI could be able to generate a novel was very intreaging. Finally I had some spare time to test this idea.</p>\\n<p>I used the following software:</p>\\n<ul>\\n<li>nvidia-docker : A docker environment that allows the GPU to be used for CUDA.</li>\\n<li>gcr.io/tensorflow/tensorflow docker image for Tensorflow, Jupyter and Python 3</li>\\n<li><a href=\\\"https://github.com/sfailsthy/char-rnn-tensorflow\\\">https://github.com/sfailsthy/char-rnn-tensorflow</a> implementation of CharNN from the previous mentioned article. It's a very clean and readable implementation. It comes complete with the Shakespeare generator and Kernel source code generator examples.</li>\\n<li>My goal was to generate a Dutch novel. Some <a href=\\\"http://ebook.gratis-downloaden.nu/\\\">Dutch novels</a> can be downloaded for free.</li>\\n<li>ebook-convert, part of the Ubuntu Calibre package.</li>\\n</ul>\\n<p>As a source I downloaded a number of free Dutch ebooks in .epub format.  Next I used ebook-convert to convert the ebook to raw ascii text:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>ebook-convert book.epub book.txt --txt-output-encoding=ascii --asciiize</code></pre>\\n      </div>\\n<p>It seemed reasonable to train on a text file which would be similar in size to the Shakespeare example. In total I concatenated six different e-books in one big text file of  about 3 megabytes, containing 500000 words. I started training the network:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>python3 train.py --input_file data/bigtextfile.txt --name bigtextfile --num_steps 100 --num_seqs 1000  --learning_rate 0.001 --max_steps 40000 --num_layers 6 --lstm_size 150</code></pre>\\n      </div>\\n<p>The parameters control the following options:</p>\\n<ul>\\n<li>1000 examples per batch</li>\\n<li>100 characters for the recurrent neural network roll back</li>\\n<li>40000 iterations</li>\\n<li>6 LSTM layers with each layer containing 150 cells.</li>\\n</ul>\\n<p>My laptop has a GTX-1060 GPU which is able to process about 0.6 batches per second. After 40000 iterations it produced the following results (in Dutch ....):</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>python3 sample.py --converter_path bigtextfile/converter.pkl --checkpoint_path bigtextfile/model/ --max_length 2000  --lstm_size=150 --num_layers=6 --start_string=\\\"Het was een prachtig idee, \\\"</code></pre>\\n      </div>\\n<pre style='white-space:pre-wrap; word-wrap:break-word;color:green'>Het was een prachtig idee, maar het was nooit en die minder honderd had hij een keurige gedachte dat hij natuurlijk het enige want ze keek om een gedachte te versloten.'\\n\\n\\t'Ik ben een schilderen. Het zou niet zo vinden.'\\n\\n\\t'Natuurlijk.' In haar bed moet ik de gezicht van de schoon keerde.\\n\\n\\t'We zijn een vruchten,' zei ik. 'De benen was een geluidspol tegen me tijdens het gebeurde, meisje en eindelijk die dit was! Waarom hij het hield voor het grote volgende bank. Iedere kast van een schor kan ik met ze ook ongelooflijk zijn!'\\n\\n\\t'Je zal je niet voor de meesten voor dat het mooisten met die gevoel ververtellen, het was in mij naar het scheemene goed veel verder en heerlijk, maar ik heb het voor het maal van het steen, maar dat is hoe doet je niet? Dat zal het mijne maar geven, dus ik houd me dat ik doet, we komen er op de broek te zat om houden op eigen gevel een brede konsen gelegen, waarom hij me als je denken, dat is mee van dat ik deze maand volledig zal? Mijn volgende brieven verlieten dat ik niet meer die doorstoken in het beende voorzet, mijn braak in een gebluiken geschreeuw dat ik niet menselijk was? De bemanning werd geleden, met zelf voor het stem van het blandende boek.\\n\\n\\t'Ja. Hij wordt die man de verhalen voor het schip weer angst. En hoe kan ze elke geval. Ik was niet meer,' zei ik open. 'Ik was niet maar op het school.'\\n\\n\\t'Ik zou me een strakke boot van een kans veel beter met elkaar vertellen dat ik nooit had opgewerkt, weet jij wat het weet na hem verslecht op dat mannen worden,' briek ze. 'Wat weet je dat wij nog wel eens gevallen?' Hij boog over hem toen ik mijn hand gebaarde van mijn verhingen. 'Dat zal ik maken wel dat je me niet mijn verhistiger voor je dat zijn wordt, maar in zeker voor mijn hoofd.'\\n\\n\\t'Die is dit met de bal van de schaal.' Ik sloot mijn hoofd.\\n\\n\\t'Ik?' room ik terriel me naar boven.\\n\\n\\tDan geven ik niet meer daar van haar ontdikte. Dat is niet alleen dat de stadper op zijn gezond bloedend was. Ik bedoel het nooit. Ik was een vriendelijk tijd dat ik nog meer \\n</pre>\\n<p>The results are already quite nice. Especially the fact that the network is able to correctly use symbols like quotes. Also interesting are the fictional Dutch sounding words it sometimes uses.</p>\\n<p>After another night of training the network generates the following text after 83000 iterations:</p>\\n<pre style='white-space:pre-wrap; word-wrap:break-word;color:green'>Waarom ben ik de verschillende kleeden.\\n\\n\\tHij had haar ging vastgemaakt.\\n\\n\\tHet was niets vast. Ik was even erover dat hij nu een voorbeeld was op de straat.\\n\\n\\t'Ik zei het niet.'\\n\\n\\tHij stopte en duwde mijn hele slanke verdriet. Ik was een kronder tegenover het gebaar van het bootje.\\n\\n\\tEen stol voelde me naar hem gelukkig af en die ze over mijn huid volgde. 'Wat is het verbijstering,' voegde ik hem, toen ik hem nog niet was. 'Het was de kleine van de moesten op de vader, dat ik hier erop. Deze brandstelling heeft me natuurlijk ook meer tegen me eens van de kans van zijn gedoop, zo heen geen gedoe verschilte vingers dat ik niet missrook was. En heb ik hem onder die boot opgeslagen dat zij niet gelaten komt, want de strofe voor die verwachting van die mensen. Maar doel dat wat je helemaal niet was, zal ik die dagen op zijn bed van de bedenking, word je het nog een boek van de stoel te bennepte. Het werkte haar niet al die hoofden en daar had ik dit er nog mijn beslaten gehad. Hij keek hem over een vreemde van de grond toen ik nog niet zich vooral had. De brief van de broeder draaide me naar de stremen. Dan stond de kleine gloed voor haar.\\n\\n'Waarom zei jij nog niet.'\\n\\n'Je mag me niet mien herstelde boeken voordat door metaal van je mee gaat.'\\n\\n'Je zou duidelijk met de boeren wel veel bij maten om te gaan en de donkere misschien is over mijn geval mogen, ze ging een snoek. De moordenaar is,' zei hij.\\n\\n'Wat is de snel van mij.'\\n\\nDe stuks voelde zijn schouder. 'Waarom werkte je nu niet maar nog wel goed?'\\n\\n'Waarom was er nog even voor het schip dit in deel van je moet horen het voor mijn man over mij maar!\\\" vroeg hij. 'Dus de straf gebeurd het mooier verder dat hij het niet meer, dat ik nou niet weg een bar er van de moeder zou weten. De kamer vindt het deed. Help die nog maar om een van de vreemde sterm een goed gedachten gezien, hard om eens een blik. Ik begrijp niet dat het woonde vissen was, maar heeft een dag om me aan het sprakken te verslagen, meer maar nog niet meer te vrienden. \\n</pre>\\n<p>The grammar is getting a bit better. Also I can now see that there is a e book about football in the training set :) Using different priming sequences I hoped to provoke more semantic understandable behaviour. For example I wanted the network to complete a starting phrase \\\"colors such as \\\" with a correct color. But I had no luck get any particular result.  I'm planning on testing different training parameters to see if I can get better results.</p>\",\"timeToRead\":5,\"excerpt\":\"Some time ago I read Karpathy's  article  on LSTM networks and text generation. The idea that AI could be able to generate a novel was very…\",\"frontmatter\":{\"title\":\"Generating text using an LSTM neural network\",\"cover\":\"/logos/book-3010727_1280.jpg\",\"date\":\"2017-12-18 20:00\",\"category\":\"Artificial intelligence\",\"tags\":[\"Artificial Intelligence\",\"LSTM\"]},\"fields\":{\"nextTitle\":\"VHDL i2s transmitter\",\"nextSlug\":\"/vhdl-i-2-s-transmitter\",\"prevTitle\":\"Real Time sound synthesis with Jupyter\",\"prevSlug\":\"/real-time-sound-synthesis-with-jupyter\",\"slug\":\"/generating-text-using-an-lstm-neural-network\"}}},\"pathContext\":{\"slug\":\"/generating-text-using-an-lstm-neural-network\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---generating-text-using-an-lstm-neural-network-14023ea4cdd2b17700fd.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<p>Some time ago I read Karpathy's <a href=\\\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\\">article</a> on LSTM networks and text generation. The idea that AI could be able to generate a novel was very intreaging. Finally I had some spare time to test this idea.</p>\\n<p>I used the following software:</p>\\n<ul>\\n<li>nvidia-docker : A docker environment that allows the GPU to be used for CUDA.</li>\\n<li>gcr.io/tensorflow/tensorflow docker image for Tensorflow, Jupyter and Python 3</li>\\n<li><a href=\\\"https://github.com/sfailsthy/char-rnn-tensorflow\\\">https://github.com/sfailsthy/char-rnn-tensorflow</a> implementation of CharNN from the previous mentioned article. It's a very clean and readable implementation. It comes complete with the Shakespeare generator and Kernel source code generator examples.</li>\\n<li>My goal was to generate a Dutch novel. Some <a href=\\\"http://ebook.gratis-downloaden.nu/\\\">Dutch novels</a> can be downloaded for free.</li>\\n<li>ebook-convert, part of the Ubuntu Calibre package.</li>\\n</ul>\\n<p>As a source I downloaded a number of free Dutch ebooks in .epub format.  Next I used ebook-convert to convert the ebook to raw ascii text:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>ebook-convert book.epub book.txt --txt-output-encoding=ascii --asciiize</code></pre>\\n      </div>\\n<p>It seemed reasonable to train on a text file which would be similar in size to the Shakespeare example. In total I concatenated six different e-books in one big text file of  about 3 megabytes, containing 500000 words. I started training the network:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>python3 train.py --input_file data/bigtextfile.txt --name bigtextfile --num_steps 100 --num_seqs 1000  --learning_rate 0.001 --max_steps 40000 --num_layers 6 --lstm_size 150</code></pre>\\n      </div>\\n<p>The parameters control the following options:</p>\\n<ul>\\n<li>1000 examples per batch</li>\\n<li>100 characters for the recurrent neural network roll back</li>\\n<li>40000 iterations</li>\\n<li>6 LSTM layers with each layer containing 150 cells.</li>\\n</ul>\\n<p>My laptop has a GTX-1060 GPU which is able to process about 0.6 batches per second. After 40000 iterations it produced the following results (in Dutch ....):</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>python3 sample.py --converter_path bigtextfile/converter.pkl --checkpoint_path bigtextfile/model/ --max_length 2000  --lstm_size=150 --num_layers=6 --start_string=\\\"Het was een prachtig idee, \\\"</code></pre>\\n      </div>\\n<pre style='white-space:pre-wrap; word-wrap:break-word;color:green'>Het was een prachtig idee, maar het was nooit en die minder honderd had hij een keurige gedachte dat hij natuurlijk het enige want ze keek om een gedachte te versloten.'\\n\\n\\t'Ik ben een schilderen. Het zou niet zo vinden.'\\n\\n\\t'Natuurlijk.' In haar bed moet ik de gezicht van de schoon keerde.\\n\\n\\t'We zijn een vruchten,' zei ik. 'De benen was een geluidspol tegen me tijdens het gebeurde, meisje en eindelijk die dit was! Waarom hij het hield voor het grote volgende bank. Iedere kast van een schor kan ik met ze ook ongelooflijk zijn!'\\n\\n\\t'Je zal je niet voor de meesten voor dat het mooisten met die gevoel ververtellen, het was in mij naar het scheemene goed veel verder en heerlijk, maar ik heb het voor het maal van het steen, maar dat is hoe doet je niet? Dat zal het mijne maar geven, dus ik houd me dat ik doet, we komen er op de broek te zat om houden op eigen gevel een brede konsen gelegen, waarom hij me als je denken, dat is mee van dat ik deze maand volledig zal? Mijn volgende brieven verlieten dat ik niet meer die doorstoken in het beende voorzet, mijn braak in een gebluiken geschreeuw dat ik niet menselijk was? De bemanning werd geleden, met zelf voor het stem van het blandende boek.\\n\\n\\t'Ja. Hij wordt die man de verhalen voor het schip weer angst. En hoe kan ze elke geval. Ik was niet meer,' zei ik open. 'Ik was niet maar op het school.'\\n\\n\\t'Ik zou me een strakke boot van een kans veel beter met elkaar vertellen dat ik nooit had opgewerkt, weet jij wat het weet na hem verslecht op dat mannen worden,' briek ze. 'Wat weet je dat wij nog wel eens gevallen?' Hij boog over hem toen ik mijn hand gebaarde van mijn verhingen. 'Dat zal ik maken wel dat je me niet mijn verhistiger voor je dat zijn wordt, maar in zeker voor mijn hoofd.'\\n\\n\\t'Die is dit met de bal van de schaal.' Ik sloot mijn hoofd.\\n\\n\\t'Ik?' room ik terriel me naar boven.\\n\\n\\tDan geven ik niet meer daar van haar ontdikte. Dat is niet alleen dat de stadper op zijn gezond bloedend was. Ik bedoel het nooit. Ik was een vriendelijk tijd dat ik nog meer \\n</pre>\\n<p>The results are already quite nice. Especially the fact that the network is able to correctly use symbols like quotes. Also interesting are the fictional Dutch sounding words it sometimes uses.</p>\\n<p>After another night of training the network generates the following text after 83000 iterations:</p>\\n<pre style='white-space:pre-wrap; word-wrap:break-word;color:green'>Waarom ben ik de verschillende kleeden.\\n\\n\\tHij had haar ging vastgemaakt.\\n\\n\\tHet was niets vast. Ik was even erover dat hij nu een voorbeeld was op de straat.\\n\\n\\t'Ik zei het niet.'\\n\\n\\tHij stopte en duwde mijn hele slanke verdriet. Ik was een kronder tegenover het gebaar van het bootje.\\n\\n\\tEen stol voelde me naar hem gelukkig af en die ze over mijn huid volgde. 'Wat is het verbijstering,' voegde ik hem, toen ik hem nog niet was. 'Het was de kleine van de moesten op de vader, dat ik hier erop. Deze brandstelling heeft me natuurlijk ook meer tegen me eens van de kans van zijn gedoop, zo heen geen gedoe verschilte vingers dat ik niet missrook was. En heb ik hem onder die boot opgeslagen dat zij niet gelaten komt, want de strofe voor die verwachting van die mensen. Maar doel dat wat je helemaal niet was, zal ik die dagen op zijn bed van de bedenking, word je het nog een boek van de stoel te bennepte. Het werkte haar niet al die hoofden en daar had ik dit er nog mijn beslaten gehad. Hij keek hem over een vreemde van de grond toen ik nog niet zich vooral had. De brief van de broeder draaide me naar de stremen. Dan stond de kleine gloed voor haar.\\n\\n'Waarom zei jij nog niet.'\\n\\n'Je mag me niet mien herstelde boeken voordat door metaal van je mee gaat.'\\n\\n'Je zou duidelijk met de boeren wel veel bij maten om te gaan en de donkere misschien is over mijn geval mogen, ze ging een snoek. De moordenaar is,' zei hij.\\n\\n'Wat is de snel van mij.'\\n\\nDe stuks voelde zijn schouder. 'Waarom werkte je nu niet maar nog wel goed?'\\n\\n'Waarom was er nog even voor het schip dit in deel van je moet horen het voor mijn man over mij maar!\\\" vroeg hij. 'Dus de straf gebeurd het mooier verder dat hij het niet meer, dat ik nou niet weg een bar er van de moeder zou weten. De kamer vindt het deed. Help die nog maar om een van de vreemde sterm een goed gedachten gezien, hard om eens een blik. Ik begrijp niet dat het woonde vissen was, maar heeft een dag om me aan het sprakken te verslagen, meer maar nog niet meer te vrienden. \\n</pre>\\n<p>The grammar is getting a bit better. Also I can now see that there is a e book about football in the training set :) Using different priming sequences I hoped to provoke more semantic understandable behaviour. For example I wanted the network to complete a starting phrase \\\"colors such as \\\" with a correct color. But I had no luck get any particular result.  I'm planning on testing different training parameters to see if I can get better results.</p>\",\"timeToRead\":5,\"excerpt\":\"Some time ago I read Karpathy's  article  on LSTM networks and text generation. The idea that AI could be able to generate a novel was very…\",\"frontmatter\":{\"title\":\"Generating text using an LSTM neural network\",\"cover\":\"/logos/book-3010727_1280.jpg\",\"date\":\"2017-12-18 20:00\",\"category\":\"Artificial intelligence\",\"tags\":[\"Artificial Intelligence\",\"LSTM\"]},\"fields\":{\"nextTitle\":\"VHDL i2s transmitter\",\"nextSlug\":\"/vhdl-i-2-s-transmitter\",\"prevTitle\":\"Real Time sound synthesis with Jupyter\",\"prevSlug\":\"/real-time-sound-synthesis-with-jupyter\",\"slug\":\"/generating-text-using-an-lstm-neural-network\"}}},\"pathContext\":{\"slug\":\"/generating-text-using-an-lstm-neural-network\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/generating-text-using-an-lstm-neural-network.json\n// module id = 465\n// module chunks = 269976026368624"],"sourceRoot":""}